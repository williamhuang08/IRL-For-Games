{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Packages\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import PPO\n",
    "from imitation.algorithms.adversarial.airl import AIRL\n",
    "from imitation.util import util\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from imitation.util.util import make_vec_env\n",
    "from imitation.rewards.reward_nets import BasicShapedRewardNet\n",
    "from imitation.rewards.reward_nets import RewardNet\n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.util import networks, util\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the true reward function's weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 50\n",
    "weights = np.random.uniform(-1, 1, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "\"\"\"\n",
    "Define the environment\n",
    "- STATES: the set of all possible observations for an agent\n",
    "- ACTIONS: the set of all possible actions an agent can take\n",
    "- STEP: determines how actions lead to changes in states\n",
    "\n",
    "In this case, the the states and actions are num_features-dimensional\n",
    "\"\"\" \n",
    "\n",
    "# Use base class from Gym\n",
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self, num_options: int = 2, weights=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_options = num_options\n",
    "        # Possible states and actions are defined as a  \"box\" with a range from [-1, 1]\n",
    "        self.observation_space = spaces.Box(low=-1, high=1, shape=(num_features,), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(num_features,), dtype=np.float32)\n",
    "        self.state = None\n",
    "        self.max_steps = 100  # Define a fixed number of steps per episode \n",
    "        \"\"\"\n",
    "        Episode:\n",
    "            - set of interactions between agent and env from starting state until terminal state\n",
    "            - agent uses interactions to improve reward\n",
    "        Steps:\n",
    "            - episodes terminate after max_steps\n",
    "        \"\"\"\n",
    "        self.current_step = 0\n",
    "\n",
    "        # Initialize weights for reward calculation\n",
    "        self.weights = weights\n",
    "\n",
    "    # Resets the environment to an initial state\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed, options=options)\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        self.state = self.observation_space.sample()\n",
    "        self.current_step = 0  # Reset the step counter\n",
    "        return self.state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # Calculate the reward based on the weights\n",
    "        reward = np.dot(self.state * action, self.weights)\n",
    "        \n",
    "        # Update the state\n",
    "        self.state = np.random.uniform(low=-1, high=1, size=(num_features,)).astype(np.float32)\n",
    "        \n",
    "        # Increment step counter\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Define the done condition based on max_steps\n",
    "        done = self.current_step >= self.max_steps\n",
    "        \n",
    "        # Since done is used, no need for truncated in this context\n",
    "        truncated = False  \n",
    "\n",
    "        info = {\n",
    "            \"obs\": self.state,\n",
    "            \"rews\": reward,\n",
    "        }\n",
    "\n",
    "        return self.state, reward, done, truncated, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the custom environment to use OpenAI's Gym API (intializes the environment with the true reward function's weights to determine state->action behavior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.register(id='CustomEnv-v0', entry_point=lambda: CustomEnv(weights=weights), max_episode_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a vectorized environment for efficient training (train multiple instances of same environment simultaneously)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "venv = util.make_vec_env(\"CustomEnv-v0\", rng=np.random.default_rng(SEED), n_envs=4, post_wrappers=[lambda env, _: RolloutInfoWrapper(env)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initalize a PPO agent to learn the environment over 10000 steps using the \"MlpPolicy\"\n",
    "PPO (Proximal Policy Optimization):\n",
    "    1. collects experiences (states, actions, rewards over some episodes)\n",
    "    2. try to find advantageous actions\n",
    "    3. Update policy\n",
    "\n",
    "Using the trained policy, collect data of the trajectories (interactions of the trained agent with the environment from some episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -3.62    |\n",
      "| time/              |          |\n",
      "|    fps             | 22177    |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 100         |\n",
      "|    ep_rew_mean          | 17.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 7091        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.063750505 |\n",
      "|    clip_fraction        | 0.463       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -70.9       |\n",
      "|    explained_variance   | -0.0106     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10          |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0749     |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 22.3        |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "expert_policy = PPO('MlpPolicy', venv, verbose=1)\n",
    "expert_policy.learn(total_timesteps=10000)\n",
    "\n",
    "# Collect rollouts\n",
    "rollouts = rollout.rollout(\n",
    "    expert_policy,\n",
    "    venv,\n",
    "    rollout.make_sample_until(min_episodes=60),\n",
    "    rng=np.random.default_rng(SEED),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_rollouts = pd.DataFrame(rollouts)\n",
    "save_rollouts.to_csv(\"rollouts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Simple Linear Model Reward Function to Learn During Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inherit from imitation's RewardNet\n",
    "class LinearRewardNet(RewardNet):\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super().__init__(observation_space, action_space)\n",
    "\n",
    "        # Define a linear layer that maps the combined state and action to a reward\n",
    "        input_dim = observation_space.shape[0] + action_space.shape[0]  # total input dimensions\n",
    "        self.linear = nn.Linear(input_dim, 1)  # Linear layer to produce a single reward value\n",
    "\n",
    "    def forward(self,\n",
    "        state: th.Tensor,  \n",
    "        action: th.Tensor,  \n",
    "        # next_state: th.Tensor,  \n",
    "        done: th.Tensor,  \n",
    "    ) -> th.Tensor:\n",
    "        # Concatenate state and action to form the input to the linear model\n",
    "        x = th.cat((state, action), dim=-1)  \n",
    "\n",
    "        # Compute the reward using the linear layer\n",
    "        reward = self.linear(x)  \n",
    "\n",
    "        # Squeeze to remove the extra dimension\n",
    "        return reward.squeeze(-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imitation library's nonlinear model\n",
    "reward_net = BasicShapedRewardNet(\n",
    "    observation_space=venv.observation_space,\n",
    "    action_space=venv.action_space,\n",
    "    normalize_input_layer=RunningNorm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_net = LinearRewardNet(\n",
    "    observation_space=venv.observation_space,\n",
    "    action_space=venv.action_space,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize AIRL to be trained on the environment, with the expert data, and the same MlpPolicy as the generator to train the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "airl_trainer = AIRL(\n",
    "    venv=venv,\n",
    "    demonstrations=rollouts,\n",
    "    demo_batch_size=60,\n",
    "    gen_algo= PPO(\n",
    "        'MlpPolicy',\n",
    "        venv,\n",
    "        verbose=1,\n",
    "    ),\n",
    "    reward_net=reward_net,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "round:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() takes 3 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[407], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m learner_rewards_before_training, _ \u001b[38;5;241m=\u001b[39m evaluate_policy(airl_trainer\u001b[38;5;241m.\u001b[39mgen_algo, venv, \u001b[38;5;241m100\u001b[39m, return_episode_rewards\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Train AIRL\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mairl_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Evaluate policy after training\u001b[39;00m\n\u001b[1;32m      7\u001b[0m learner_rewards_after_training, _ \u001b[38;5;241m=\u001b[39m evaluate_policy(airl_trainer\u001b[38;5;241m.\u001b[39mgen_algo, venv, \u001b[38;5;241m100\u001b[39m, return_episode_rewards\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tobin/lib/python3.8/site-packages/imitation/algorithms/adversarial/common.py:454\u001b[0m, in \u001b[0;36mAdversarialTrainer.train\u001b[0;34m(self, total_timesteps, callback)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m n_rounds \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, (\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo updates (need at least \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen_train_timesteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m timesteps, have only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_timesteps=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_timesteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m )\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, n_rounds), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mround\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen_train_timesteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_disc_updates_per_round):\n\u001b[1;32m    456\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m networks\u001b[38;5;241m.\u001b[39mtraining(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_train):\n\u001b[1;32m    457\u001b[0m             \u001b[38;5;66;03m# switch to training mode (affects dropout, normalization)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tobin/lib/python3.8/site-packages/imitation/algorithms/adversarial/common.py:414\u001b[0m, in \u001b[0;36mAdversarialTrainer.train_gen\u001b[0;34m(self, total_timesteps, learn_kwargs)\u001b[0m\n\u001b[1;32m    411\u001b[0m     learn_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39maccumulate_means(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgen\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 414\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen_algo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlearn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_global_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    422\u001b[0m gen_trajs, ep_lens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvenv_buffering\u001b[38;5;241m.\u001b[39mpop_trajectories()\n",
      "File \u001b[0;32m~/anaconda3/envs/tobin/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tobin/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tobin/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:195\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[1;32m    193\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 195\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tobin/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tobin/lib/python3.8/site-packages/imitation/rewards/reward_wrapper.py:110\u001b[0m, in \u001b[0;36mRewardVecEnvWrapper.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m     obs_fixed\u001b[38;5;241m.\u001b[39mappend(types\u001b[38;5;241m.\u001b[39mmaybe_wrap_in_dictobs(single_obs))\n\u001b[1;32m    105\u001b[0m obs_fixed \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    106\u001b[0m     types\u001b[38;5;241m.\u001b[39mDictObs\u001b[38;5;241m.\u001b[39mstack(obs_fixed)\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs, types\u001b[38;5;241m.\u001b[39mDictObs)\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mstack(obs_fixed)\n\u001b[1;32m    109\u001b[0m )\n\u001b[0;32m--> 110\u001b[0m rews \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreward_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_obs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_unwrap_dictobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_fixed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdones\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rews) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(obs), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust return one rew for each env\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    117\u001b[0m done_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(dones, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;28mlen\u001b[39m(dones),))\n",
      "File \u001b[0;32m~/anaconda3/envs/tobin/lib/python3.8/site-packages/imitation/rewards/reward_nets.py:204\u001b[0m, in \u001b[0;36mRewardNet.predict_processed\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the processed rewards for a batch of transitions without gradients.\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03mDefaults to calling `predict`. Subclasses can override this to normalize or\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    Computed processed rewards of shape `(batch_size,`).\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m kwargs\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tobin/lib/python3.8/site-packages/imitation/rewards/reward_nets.py:175\u001b[0m, in \u001b[0;36mRewardNet.predict\u001b[0;34m(self, state, action, next_state, done)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    157\u001b[0m     state: np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m     done: np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[1;32m    161\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute rewards for a batch of transitions without gradients.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    Converting th.Tensor rewards from `predict_th` to NumPy arrays.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03m        Computed rewards of shape `(batch_size,)`.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     rew_th \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_th\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rew_th\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n",
      "File \u001b[0;32m~/anaconda3/envs/tobin/lib/python3.8/site-packages/imitation/rewards/reward_nets.py:150\u001b[0m, in \u001b[0;36mRewardNet.predict_th\u001b[0;34m(self, state, action, next_state, done)\u001b[0m\n\u001b[1;32m    143\u001b[0m state_th, action_th, next_state_th, done_th \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(\n\u001b[1;32m    144\u001b[0m     state,\n\u001b[1;32m    145\u001b[0m     action,\n\u001b[1;32m    146\u001b[0m     next_state,\n\u001b[1;32m    147\u001b[0m     done,\n\u001b[1;32m    148\u001b[0m )\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 150\u001b[0m     rew_th \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstate_th\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_th\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state_th\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone_th\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m rew_th\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m state\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rew_th\n",
      "File \u001b[0;32m~/anaconda3/envs/tobin/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tobin/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() takes 3 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "learner_rewards_before_training, _ = evaluate_policy(airl_trainer.gen_algo, venv, 100, return_episode_rewards=True)\n",
    "\n",
    "# Train AIRL\n",
    "airl_trainer.train(total_timesteps=20000)\n",
    "\n",
    "# Evaluate policy after training\n",
    "learner_rewards_after_training, _ = evaluate_policy(airl_trainer.gen_algo, venv, 100, return_episode_rewards=True)\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(\"Rewards before training:\", learner_rewards_before_training)\n",
    "print(\"Rewards after training:\", learner_rewards_after_training)\n",
    "\n",
    "print(\"Mean Rewards before training:\", np.mean(learner_rewards_before_training))\n",
    "print(\"Mean Rewards after training:\", np.mean(learner_rewards_after_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 2 required positional arguments: 'next_state' and 'done'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[402], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m actions_tensor \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mtensor(actions, dtype\u001b[38;5;241m=\u001b[39mth\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Use the reward network to predict rewards\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m predicted_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mreward_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Calculate the Mean Squared Error\u001b[39;00m\n\u001b[1;32m     24\u001b[0m mse \u001b[38;5;241m=\u001b[39m mean_squared_error(true_rewards, predicted_rewards)\n",
      "File \u001b[0;32m~/anaconda3/envs/tobin/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tobin/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 2 required positional arguments: 'next_state' and 'done'"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming rollouts is your collected rollout data\n",
    "# Extract states, actions, and rewards from rollouts\n",
    "states = obs_array  # Assuming this is an array of shape (num_timesteps, num_features)\n",
    "actions = acts_array  # Assuming this is an array of shape (num_timesteps, num_features)\n",
    "true_rewards = rews_array  # Assuming rewards are stored in this key\n",
    "\n",
    "# Create an instance of the linear reward network\n",
    "observation_space = spaces.Box(low=-1, high=1, shape=(num_features,), dtype=np.float32)\n",
    "action_space = spaces.Box(low=-1, high=1, shape=(num_features,), dtype=np.float32)\n",
    "reward_net = LinearRewardNet(observation_space, action_space)\n",
    "\n",
    "# Convert states and actions to tensors\n",
    "states_tensor = th.tensor(states, dtype=th.float32)\n",
    "actions_tensor = th.tensor(actions, dtype=th.float32)\n",
    "\n",
    "# Use the reward network to predict rewards\n",
    "predicted_rewards = reward_net(states_tensor, actions_tensor).detach().numpy()\n",
    "\n",
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(true_rewards, predicted_rewards)\n",
    "\n",
    "# Print the MSE\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import csv\n",
    "\n",
    "def parse_array_string(array_str):\n",
    "    # Remove the prefix \"array(\" and the suffix \")\" if present\n",
    "    if 'array(' in array_str:\n",
    "        array_str = array_str[array_str.index('(') + 1: array_str.rindex(')')]\n",
    "    \n",
    "    # Replace \" \" with \", \" for easier conversion\n",
    "    array_str = array_str.replace(' ', ', ')\n",
    "    \n",
    "    # Convert the cleaned string to a list and then to a NumPy array\n",
    "    return np.array(eval(array_str))\n",
    "\n",
    "# Replace 'data.csv' with the path to your CSV file\n",
    "csv_file_path = 'data.csv'\n",
    "\n",
    "# Initialize lists to hold the extracted data\n",
    "obs_list = []\n",
    "acts_list = []\n",
    "rews_list = []\n",
    "\n",
    "# Read the CSV file\n",
    "with open('./rollouts.csv', mode='r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    \n",
    "    for row in reader:\n",
    "        # Extract 'obs', 'acts', and 'rews' columns\n",
    "        obs = row['obs']\n",
    "        acts = row['acts']\n",
    "        rews = row['rews']\n",
    "\n",
    "        # Parse and append to lists if they are not empty\n",
    "        if obs:  # Check if obs is not empty\n",
    "            obs_list.append(obs_array)\n",
    "        \n",
    "        if acts:  # Check if acts is not empty\n",
    "            acts_list.append(acts_array)\n",
    "\n",
    "        if rews:  # Check if rews is not empty\n",
    "            rews_list.append(rews_array)\n",
    "\n",
    "# Convert lists to NumPy arrays if needed\n",
    "obs_array = np.array(obs_list)\n",
    "acts_array = np.array(acts_list)\n",
    "rews_array = np.array(rews_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tobin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
